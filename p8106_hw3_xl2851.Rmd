---
title: "p8106_hw3_xl2851"
author: "xinran"
date: "4/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
library(caret)
library(glmnet)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(AppliedPredictiveModeling)
library(tidyverse)
```

# Load Data
```{r}
data("Weekly")
weekly = Weekly %>% 
  janitor::clean_names()
head(weekly)
```

### (a) Produce some graphical summaries of the Weekly data.

```{r}
# start from some simple visualization of weekly
# density plot
transparentTheme(trans = .4)
featurePlot(x = weekly[, 1:8], 
            y = weekly$direction,
            scales = list(x=list(relation="free"), 
                        y=list(relation="free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))
# pairs scatterplot
pairs(weekly)
```

### (b) Use the full data set to perform a logistic regression with Direction as the response and the five Lag variables plus Volume as predictors. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
glm.fit <- glm(direction ~ lag1 + lag2 + lag3 + lag4 + lag5 + volume, 
               data = weekly,
               family = binomial)
summary(glm.fit)
```

Based on the outputs, lag2 is significant with p-value = 0.0296 < 0.05.

### (c) Compute the confusion matrix and overall fraction of correct predictions. Briefly explain what the confusion matrix is telling you.

```{r}
# first consider the Bayes classifier(cutoff = 0.5) and evaluate its performance on the data
test.pred.prob = predict(glm.fit,
                         type = "response")
test.pred = rep("Down", length(test.pred.prob))
test.pred[test.pred.prob > 0.5] = "Up"
confusionMatrix(data = as.factor(test.pred),
                reference = weekly$direction,
                positive = "Up")
```

Based on the output, the confusion matrix provides us with following results:  
1. **Accuracy = 0.5611:** provides the probability of the correct classifer, which is the overall fraction of correct predictions. ((TP+TN)/n = (54+557)/1089)  
2. **NIR = 0.5556:** provides the larger proportion of total positive observation vs. the proportion of total negative observations. (max((TP+FP)/n, (FN+TN)/n))  
3. **Kappa = 0.035:** measures the agreement between classification and truth values. A kappa value closed to 1 meaning a good performance of the model.  
4. **Sensitivity = 0.9207:** measures the proportion of actual positives that are correctly identified. (TP/(TP+FN))  
5. **Specificity = 0.1116:** measures the proportion of actual negatives that are correctly identifed. (TN/(FP+TN))

### (d) Plot the ROC curve using the predicted probability from logistic regression and report the AUC.

# (d) Plot the ROC curve using the predicted probability from logistic regression and report the AUC.
```{r}
roc.glm <- roc(weekly$direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```

AUC = 0.554.

### (e) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors. Plot the ROC curve using the held out data (that is, the data from 2009 and 2010) and report the AUC.

# (e) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors. Plot the ROC curve using the held out data (that is, the data from 2009 and 2010) and report the AUC.
```{r}
# divide data into train and test
train_data = subset(weekly, year <= 2008)
test_data = subset(weekly, year >= 2009)
# fit regression using training data
glm.fit_tr = glm(direction ~ lag1 + lag2, 
               data = train_data,
               family = binomial)
# predict using test data
test.pred.prob2 = predict(glm.fit_tr,
                          newdata = test_data,
                          type = "response")
test.pred2 = rep("Down", length(test.pred.prob2))
test.pred2[test.pred.prob2 > 0.5] = "Up"
# plot ROC curve and report AUC
roc.glm2 <- roc(test_data$direction, test.pred.prob2)
plot(roc.glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm2), col = 4, add = TRUE)
```

AUC = 0.556.


### (f) Repeat (e) using LDA and QDA.

## LDA
```{r}
# fit model on trainning and predict on test
lda.fit = lda(direction ~ lag1 + lag2,
              data = train_data)
lda.pred = predict(lda.fit, 
                   newdata = test_data)
# plot ROC curve
roc.lda = roc(test_data$direction, lda.pred$posterior[,2],
           levels = c("Down", "Up"))
plot(roc.lda, legacy.axes = T, print.auc = T)
```
AUC = 0.557.

## QDA
```{r}
# fit model on trainning and predict on test
qda.fit = qda(direction ~ lag1 + lag2,
              data = train_data)
qda.pred = predict(qda.fit, 
                   newdata = test_data)
# plot ROC curve
roc.qda = roc(test_data$direction, qda.pred$posterior[,2],
           levels = c("Down", "Up"))
plot(roc.qda, legacy.axes = T, print.auc = T)
```
AUC = 0.529.

### (g) Repeat (e) using KNN. Briefly discuss your results

```{r}
# fit KNN model on training data
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
model.knn <- train(x = train_data[2:3],
                   y = train_data$direction,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(1,200,by=5)),
                   trControl = ctrl)
summary(model.knn)
# predict on test data
knn.pred = predict(model.knn, 
                   newdata = test_data,
                   type = "prob")
# plot ROC curve
roc.knn = roc(test_data$direction, knn.pred$Down,
              levels = c("Down", "Up"))
plot(roc.qda, legacy.axes = T, print.auc = T)
```

Based on the results, AUC for KNN model is 0.529, meaning area under the ROC curve under KNN model is 52.9%. Comparing all fitted models, LDA provides the largest AUC among logistic regression, QDA, LDA and KNN. Hence, LDA tends to indicate a model with good predicting performace. However, further test on training data performance need to be conducted using CV.
